<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Maruf M. </title> <meta name="author" content="Maruf M."> <meta name="description" content="Personal website "> <meta name="keywords" content="m maruf, maruf, research, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/samMarfy.jpg?bb8930429cf840c2eae89dfc55701c9d"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://sammarfy.github.io/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <div class="navbar-brand social"> <a href="mailto:%6D%61%72%75%66%6D@%76%74.%65%64%75" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=SiY3Sz4AAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/sammarfy" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/samu14792" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://twitter.com/sammarfy" title="X" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-x-twitter"></i></a> <a href="https://dblp.org/pid/268/8054.html" title="DBLP" rel="external nofollow noopener" target="_blank"><i class="ai ai-dblp"></i></a> </div> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/assets/pdf/Maruf_CV.pdf">CV </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Maruf</span> M. </h1> <p class="desc"><a href="#">Ph.D. Candidate @ Virginia Tech. Machine Learning Researcher.</a></p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/profile_pic-480.webp 480w,/assets/img/profile_pic-800.webp 800w,/assets/img/profile_pic-1400.webp 1400w," sizes="(min-width: 930px) 270.0px, (min-width: 576px) 30vw, 95vw" type="image/webp"> <img src="/assets/img/profile_pic.jpg?4a11e9922be0f7776cd05250d7148cbd" class="img-fluid z-depth-1 rounded-circle" width="100%" height="auto" alt="profile_pic.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div class="clearfix"> <p>Hi there! I am a final-year Ph.D. candidate in the Computer Science Department at Virginia Tech. I am working on integrating multimodal data, including knowledge graphs, images, and textual information, into machine learning models, particularly Vision Language Models, to enhance their applicability in scientific domains.</p> <p>Under the supervision of <a href="https://people.cs.vt.edu/karpatne/" rel="external nofollow noopener" target="_blank">Prof. Anuj Karpatne</a> in <a href="https://kgml-lab.github.io/" rel="external nofollow noopener" target="_blank">KGML lab</a>, my work extends to the development of Knowledge-Guided Machine Learning approaches for graph representation learning, GANs, segmentation, and imageomics, with a commitment to drive forward the convergence of Computer Vision, Graph Neural Networks, Deep Learning, and Natural Language Processing in scientific research.</p> <p>I’ve also gathered experience in industry as a <em>research intern at Amazon (2022) and Qualcomm (2021)</em>. In 2023, I was awarded the <a href="https://cs.vt.edu/Graduate/Funding/fellowships.html" rel="external nofollow noopener" target="_blank">Kafura Graduate Fellowship</a> from the Computer Science department at Virginia Tech.</p> <p>Prior to joining the Ph.D. program, I completed my Master’s at <a href="https://cs.vt.edu/" rel="external nofollow noopener" target="_blank">CS@VT</a>. Previously, I had completed my Bachelor’s in Computer Science and Engineering from <a href="https://www.buet.ac.bd/web/#/" rel="external nofollow noopener" target="_blank">Bangladesh University of Engineering and Technology (BUET)</a>. Apart from academia, I love reading books, playing Chess and Table tennis.</p> </div> <h2> <a href="/news/" style="color: inherit">Recent News!</a> </h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 15%">Aug 2024</th> <td> VLM4Bio is in arxiv!! Check it out <a href="https://arxiv.org/abs/2408.16176" rel="external nofollow noopener" target="_blank">here</a> </td> </tr> <tr> <th scope="row" style="width: 15%">Aug 2024</th> <td> Our FishVista paper pre-print is available on arxiv! Check it out <a href="https://arxiv.org/abs/2407.08027" rel="external nofollow noopener" target="_blank">here</a>. </td> </tr> <tr> <th scope="row" style="width: 15%">Jul 2024</th> <td> Yay <img class="emoji" title=":tada:" alt=":tada:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f389.png" height="20" width="20"> PhyloDiffusion paper is accepted at ECCV 2024! Check it out: <a href="https://arxiv.org/pdf/2408.00160v1" rel="external nofollow noopener" target="_blank">here</a>. </td> </tr> <tr> <th scope="row" style="width: 15%">Dec 2023</th> <td> 3 papers are accepted at the first workshop on Imageomics at AAAI, 2023. Here is my talk on <a href="https://drive.google.com/file/d/1cfrU5QVvL7hrtLLSRRGdFQnCT9Kq9z9f/view?usp=sharing" rel="external nofollow noopener" target="_blank">“Are Pre-trained Vision Language Models (VLMs) Decent Zero-shot Predictors in Scientific Contexts?”</a> </td> </tr> <tr> <th scope="row" style="width: 15%">Aug 2023</th> <td> Check out our paper on weakly supervised semantic segmentation. It’s on <a href="https://arxiv.org/abs/2308.11052" rel="external nofollow noopener" target="_blank">arxiv</a>. </td> </tr> <tr> <th scope="row" style="width: 15%">May 2022</th> <td> Happy for my applied scientist internship opportunity @ Amazon. </td> </tr> <tr> <th scope="row" style="width: 15%">Dec 2021</th> <td> Our work on structured pruning is accepted at NeurIPS 2021. Yay <img class="emoji" title=":tada:" alt=":tada:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f389.png" height="20" width="20"> Check out the paper <a href="https://arxiv.org/pdf/2110.00684" rel="external nofollow noopener" target="_blank">here</a>. </td> </tr> <tr> <th scope="row" style="width: 15%">May 2021</th> <td> Excited to a Machine Learning Intern in WRD team @ Qualcomm. </td> </tr> <tr> <th scope="row" style="width: 15%">Mar 2021</th> <td> PID-GAN paper is accepted at KDD’21. Yay <img class="emoji" title=":tada:" alt=":tada:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f389.png" height="20" width="20"> Check out the <a href="https://arxiv.org/pdf/2106.02993" rel="external nofollow noopener" target="_blank">paper</a>. </td> </tr> <tr> <th scope="row" style="width: 15%">Aug 2020</th> <td> Our paper on Graph Representation Learning using Distance-aware Negative Sampling is accepted at SDM 2020. Yay <img class="emoji" title=":tada:" alt=":tada:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f389.png" height="20" width="20">. Check it out <a href="https://arxiv.org/pdf/2007.01423" rel="external nofollow noopener" target="_blank">here</a>. </td> </tr> </table> </div> </div> <h2> <a href="/publications/" style="color: inherit">Selected Publications</a> </h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NeurIPS 24</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/vlm4bio-480.webp 480w,/assets/img/publication_preview/vlm4bio-800.webp 800w,/assets/img/publication_preview/vlm4bio-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="/assets/img/publication_preview/vlm4bio.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="vlm4bio.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="maruf24vlm4bio" class="col-sm-8"> <div class="title">VLM4Bio: A Benchmark Dataset to Evaluate Pretrained Vision-Language Models for Trait Discovery from Biological Images</div> <div class="author"> <em>M. Maruf</em>, A. Daw, K. S. Mehrab, and <span class="more-authors" title="click to view 19 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '19 more authors' ? 'H.B. Manogaran, A. Neog, M. Sawhney, Mridul Khurana, James P. Balhoff, Yasin Bakis, Bahadir Altintas, Matthew J. Thompson, Elizabeth G. Campolongo, Josef C. Uyeda, Hilmar Lapp, Henry L. Bart, Paula M. Mabee, Yu Su, Wei-Lun Chao, Charles Stewart, Tanya Berger-Wolf, Wasila Dahdul, Anuj Karpatne' : '19 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">19 more authors</span> </div> <div class="periodical"> <em>In Advances in Neural Information Processing Systems (NeurIPS)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2408.16176" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://github.com/sammarfy/VLM4Bio" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Images are increasingly becoming the currency for documenting biodiversity on the planet, providing novel opportunities for accelerating scientific discoveries in the field of organismal biology, especially with the advent of large vision-language models (VLMs). We ask if pre-trained VLMs can aid scientists in answering a range of biologically relevant questions without any additional fine-tuning. In this paper, we evaluate the effectiveness of 12 state-of-the-art (SOTA) VLMs in the field of organismal biology using a novel dataset, VLM4Bio, consisting of 469K question-answer pairs involving 30K images from three groups of organisms: fishes, birds, and butterflies, covering five biologically relevant tasks. We also explore the effects of applying prompting techniques and tests for reasoning hallucination on the performance of VLMs, shedding new light on the capabilities of current SOTA VLMs in answering biologically relevant questions using images.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NeurIPS 24</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/hcompnet-480.webp 480w,/assets/img/publication_preview/hcompnet-800.webp 800w,/assets/img/publication_preview/hcompnet-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="/assets/img/publication_preview/hcompnet.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="hcompnet.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="harish24hcompnet" class="col-sm-8"> <div class="title">What Do You See in Common? Learning Hierarchical Prototypes over Tree-of-Life to Discover Evolutionary Traits </div> <div class="author"> H. B. Manogaran, <em>M. Maruf</em>, A. Daw, and <span class="more-authors" title="click to view 10 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '10 more authors' ? 'K. S. Mehrab, Caleb P. Charpentier, Josef C. Uyeda, Wasila Dahdul, Matthew J., Elizabeth G. Campolongo, Kaiya L. Provost, Paula M. Mabee, Hilmar Lapp, Anuj Karpatne' : '10 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">10 more authors</span> </div> <div class="periodical"> <em>In Advances in Neural Information Processing Systems (NeurIPS)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2405.13063" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://github.com/sammarfy/VLM4Bio" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>A grand challenge in biology is to discover evolutionary traits—features of organisms common to a group of species with a shared ancestor in the tree of life (also referred to as phylogenetic tree). With the growing availability of image repositories in biology, there is a tremendous opportunity to discover evolutionary traits directly from images in the form of a hierarchy of prototypes. However, current prototype-based methods are mostly designed to operate over a flat structure of classes and face several challenges in discovering hierarchical prototypes, including the issue of learning over-specific features at internal nodes. To overcome these challenges, we introduce the framework of Hierarchy aligned Commonality through Prototypical Networks (HComP-Net). We empirically show that HComP-Net learns prototypes that are accurate, semantically consistent, and generalizable to unseen species in comparison to baselines on birds, butterflies, and fish datasets.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ECCV 24</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/phyloDiffusion-480.webp 480w,/assets/img/publication_preview/phyloDiffusion-800.webp 800w,/assets/img/publication_preview/phyloDiffusion-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="/assets/img/publication_preview/phyloDiffusion.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="phyloDiffusion.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="khurana_hierarchical" class="col-sm-8"> <div class="title">Hierarchical Conditioning of Diffusion Models Using Tree-of-Life for Studying Species Evolution</div> <div class="author"> Mridul Khurana, Arka Daw, <em>M. Maruf</em>, and <span class="more-authors" title="click to view 12 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '12 more authors' ? 'Josef C. Uyeda, Wasila Dahdul, Caleb Charpentier, Yasin Bakis, Henry L. Bart, Paula M. Mabee, Hilmar Lapp, James P. Balhoff, Wei-Lun Chao, Charles Stewart, Tanya Berger-Wolf, Anuj Karpatne' : '12 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">12 more authors</span> </div> <div class="periodical"> <em>In Proceedings of ECCV</em>, Milan, Italy, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2408.00160v1" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://imageomics.github.io/phylo-diffusion" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>A central problem in evolutionary biology is to explore the genetic basis of evolutionary changes in the traits of organisms, such as fin structures in fish or beak colors in birds. With the growing availability of large-scale image repositories in biology and recent advances in generative modeling, there is an opportunity to study changes in evolutionary traits of species automatically from images. We introduce a novel Hierarchical Embedding (HIER-Embed) strategy to encode the evolutionary information of a species as a composition of encodings learned at every internal node in the phylogenetic tree. We use HIER-Embeddings to condition latent diffusion models to generate synthetic images of species. Further, we introduce two novel types of perturbation operations: trait masking and trait swapping, similar in spirit to gene knockout experiments, that enable us to analyze novel changes in evolutionary traits acquired at different levels of phylogeny.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#009f36"> <div>Vision</div> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/ws3-480.webp 480w,/assets/img/publication_preview/ws3-800.webp 800w,/assets/img/publication_preview/ws3-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="/assets/img/publication_preview/ws3.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="ws3.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="maruf2023beyond" class="col-sm-8"> <div class="title">Beyond Discriminative Regions: Saliency Maps as Alternatives to CAMs for Weakly Supervised Semantic Segmentation</div> <div class="author"> <em>M. Maruf</em>, Arka Daw, Amartya Dutta, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Jie Bu, Anuj Karpatne' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>In arXiv</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2308.11052" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://github.com/sammarfy/WS3_lib" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>A central goal in deep learning is to learn compact representations of features at every layer of a neural network, which is useful for both unsupervised representation learning and structured network pruning. While there is a growing body of work in structured pruning, current state-of-the-art methods suffer from two key limitations: (i) instability during training, and (ii) need for an additional step of fine-tuning, which is resource-intensive. At the core of these limitations is the lack of a systematic approach that jointly prunes and refines weights during training in a single stage, and does not require any fine-tuning upon convergence to achieve state-of-the-art performance. We present a novel single-stage structured pruning method termed DiscriminAtive Masking (DAM). The key intuition behind DAM is to discriminatively prefer some of the neurons to be refined during the training process, while gradually masking out other neurons. We show that our proposed DAM approach has remarkably good performance over various applications, including dimensionality reduction, recommendation system, graph representation learning, and structured pruning for image classification. We also theoretically show that the learning objective of DAM is directly related to minimizing the L0 norm of the masking layer.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NeurIPS 21</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/dam-480.webp 480w,/assets/img/publication_preview/dam-800.webp 800w,/assets/img/publication_preview/dam-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="/assets/img/publication_preview/dam.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="dam.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="bu2021learning" class="col-sm-8"> <div class="title">Learning Compact Representations of Neural Networks using DiscriminAtive Masking (DAM)</div> <div class="author"> Jie Bu<sup>*</sup>, Arka Daw<sup>*</sup>, <em>M. Maruf<sup>*</sup></em>, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Anuj Karpatne' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In Advances in Neural Information Processing Systems (NeurIPS)</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2110.00684" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://github.com/jayroxis/dam-pytorch" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>A central goal in deep learning is to learn compact representations of features at every layer of a neural network, which is useful for both unsupervised representation learning and structured network pruning. While there is a growing body of work in structured pruning, current state-of-the-art methods suffer from two key limitations: (i) instability during training, and (ii) need for an additional step of fine-tuning, which is resource-intensive. At the core of these limitations is the lack of a systematic approach that jointly prunes and refines weights during training in a single stage, and does not require any fine-tuning upon convergence to achieve state-of-the-art performance. We present a novel single-stage structured pruning method termed DiscriminAtive Masking (DAM). The key intuition behind DAM is to discriminatively prefer some of the neurons to be refined during the training process, while gradually masking out other neurons. We show that our proposed DAM approach has remarkably good performance over various applications, including dimensionality reduction, recommendation system, graph representation learning, and structured pruning for image classification. We also theoretically show that the learning objective of DAM is directly related to minimizing the L0 norm of the masking layer.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">KDD 21</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/pidgan-480.webp 480w,/assets/img/publication_preview/pidgan-800.webp 800w,/assets/img/publication_preview/pidgan-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="/assets/img/publication_preview/pidgan.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="pidgan.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="daw2021pid" class="col-sm-8"> <div class="title">PID-GAN: A GAN Framework based on a Physics-informed Discriminator for Uncertainty Quantification with Physics</div> <div class="author"> Arka Daw<sup>*</sup>, <em>M. Maruf<sup>*</sup></em>, and Anuj Karpatne </div> <div class="periodical"> <em>In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining (KDD)</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2106.02993" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://github.com/arkadaw9/PID-GAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>As applications of deep learning (DL) continue to seep into critical scientific use-cases, the importance of performing uncertainty quantification (UQ) with DL has become more pressing than ever before. In scientific applications, it is also important to inform the learning of DL models with knowledge of physics of the problem to produce physically consistent and generalized solutions. This is referred to as the emerging field of physics-informed deep learning (PIDL). We consider the problem of developing PIDL formulations that can also perform UQ. To this end, we propose a novel physics-informed GAN architecture, termed PID-GAN, where the knowledge of physics is used to inform the learning of both the generator and discriminator models, making ample use of unlabeled data instances. We show that our proposed PID-GAN framework does not suffer from imbalance of generator gradients from multiple loss terms as compared to state-of-the-art. We also empirically demonstrate the efficacy of our proposed framework on a variety of case studies involving benchmark physics-based PDEs as well as imperfect physics.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">SDM 20</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/dns-480.webp 480w,/assets/img/publication_preview/dns-800.webp 800w,/assets/img/publication_preview/dns-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="/assets/img/publication_preview/dns.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="dns.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="maruf2021maximizing" class="col-sm-8"> <div class="title">Maximizing cohesion and separation in graph representation learning: A distance-aware negative sampling approach</div> <div class="author"> <em>M. Maruf</em>, and Anuj Karpatne </div> <div class="periodical"> <em>In Proceedings of the 2021 SIAM International Conference on Data Mining</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2007.01423" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://github.com/sammarfy/DNS" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>The objective of unsupervised graph representation learning (GRL) is to learn a low-dimensional space of node embeddings that reflect the structure of a given unlabeled graph. Existing algorithms for this task rely on negative sampling objectives that maximize the similarity in node embeddings at nearby nodes (referred to as "cohesion") by maintaining positive and negative corpus of node pairs. While positive samples are drawn from node pairs that co-occur in short random walks, conventional approaches construct negative corpus by uniformly sampling random pairs, thus ignoring valuable information about structural dissimilarity among distant node pairs (referred to as "separation"). In this paper, we present a novel Distance-aware Negative Sampling (DNS) which maximizes the separation of distant node-pairs while maximizing cohesion at nearby node-pairs by setting the negative sampling probability proportional to the pair-wise shortest distances. Our approach can be used in conjunction with any GRL algorithm and we demonstrate the efficacy of our approach over baseline negative sampling methods over downstream node classification tasks on a number of benchmark datasets and GRL algorithms.</p> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%6D%61%72%75%66%6D@%76%74.%65%64%75" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=SiY3Sz4AAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/sammarfy" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/samu14792" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://twitter.com/sammarfy" title="X" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-x-twitter"></i></a> <a href="https://dblp.org/pid/268/8054.html" title="DBLP" rel="external nofollow noopener" target="_blank"><i class="ai ai-dblp"></i></a> </div> <div class="contact-note"></div> </div> </article> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2024 Maruf M.. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>